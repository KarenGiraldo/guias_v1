{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: creación de ETLs incrementales con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\t\n",
    "\t¿Qué aprenderá?\n",
    "    En este tutorial aprenderá cómo puede usar PySpark para crear ETLs con historia en las dimensiones y con datos cargados previamente en la bodega de datos. \n",
    "\t\n",
    "\t¿Qué construirá? \n",
    "\tConstruirá una nueva versión del ETL, a partir del ETL del taller anterior. Esta vez incluyendo el manejo de historia para algunas de las dimensiones del modelo multidimensional.\n",
    "    \n",
    "\t¿Para qué?\n",
    "\tDentro  de  procesos  de  ETL,  es común  que se  presenten  dimensiones que puedan  presentar cambios a través del tiempo para las cuales es necesario tener un plan de manejo de historia. Por lo tanto, es esencial saber cómo realizar este manejo en las distintas herramientas de ETLs y por supuesto cargas cuando la bodega de datos tiene datos cargados.\n",
    "    \n",
    "    ¿Qué necesita?\n",
    "    1. Python 3 con pip instalado\n",
    "    2. Jupyter Lab\n",
    "    3. Paquetes: Pyspark (3.0.1) y pandas (1.2.1)\n",
    "    4. Controlador J de MySQL\n",
    "    5. Servidor SQL con base de datos \"WWImporters\" que contenga la dimension \"Producto\"\n",
    "    6. Archivo con la información histórica de productos:\n",
    "[ReporteNum2](./ReporteNum2_Productos.csv)\n",
    "\n",
    "    7. Archivo con la información de fechas.\n",
    "\n",
    "[Fecha](./Fecha.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En  este tutorial,  utilizará PySpark para crear y ejecutar un ETL incremental con manejo de historia. Para completar esta actividad, es necesario que haya completado la actividad previa de creación de un ETL, pues se hará una extensión de dicho proceso. Se presenta a continuación el modelo multidimensional sin historia, cuyos datos conforman la base que se tiene actualmente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Modelo ordenes](./WWI_modelo_ordenes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este tutorial es transformar el modelo anterior de forma que incorpore nuevos datos y en particular el manejo de historia de la dimensión Producto, para lo anterior se propone manterner la dimensión original y crear una nueva para el manejo de historia (<i>ProductoHistoria</i>), se espera que despues de este proceso el modelo multidimensional sea el siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Modelo ETL](./Img/ModeloHistoriaTutorial.png)\n",
    "\n",
    "En el caso de la primera carga los valores por defecto para los atributos version, FechaInicial, FechaFinal y ID_Producto serán la fecha en la que se hace la carga, 2199-12-31 y el id de la dimension producto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ETL Incremental para Productos\n",
    "WideWorldImporters ha determinado que existe la posibilidad de que un <i>Producto</i> cambie su precio unitario. En vista de esto, se debe modificar el proceso ETL para manejar los cambios. Se ha decidido que el manejo que se le debe dar a la historia es **tipo 2**. Este tipo implica que la tabla tendrá 3 columnas nuevas: 1. fecha de inicio, 2. fin de vigencia del registro y 3. version, la última es un indicador de cuántos registros hay por cada producto. Un nuevo registro es ingresado cuando se genera un cambio en alguno de los atributos de los que se lleva historia. Esto lleva adicionalmente, a actualizar la columna que maneja la vigencia del Producto (fecha fin).\n",
    "\n",
    "De acuerdo con esto, se debe añadir una serie de  transformaciones en el ETL al final para la dimensión de Producto, es decir, en el bloque 4 del diseño del ETL. Estas transformaciones incluyen las siguientes operaciones: \n",
    "<ol>\n",
    "<li>Leer los datos ya existentes en la bodega o DataWarehouse (Base de datos Estudiante_i) y traer la última versión de cada <i>Producto</i>.</li>\n",
    "<li>Identificar de los Productos reportados por en el negocio en un archivo csv (<i>ReporteNum2_Productos.csv</i>) con corte 2015-01-01, cuáles son nuevos e insertarlos en la bodega o DataWarehouse (Base de datos Estudiante_i) en la tabla ProductoHistoria.</li>\n",
    "<li>Identificar los Productos que cambiaron de precio, para insertar un nuevo registro con el nuevo precio y modificar el registro existente, cambiando el valor del atributo <i>Fecha final</i>.</li>\n",
    "</ol>\n",
    "\n",
    "Este tutorial utiliza el ETL básico y se cambia el código del bloque 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración servidor base de datos transaccional\n",
    "# Utilizar el usuario asignado en el curso\n",
    "db_user = 'Estudiante_19_202413'\n",
    "db_psswd = 'aabb1122'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_db_connection_string = 'jdbc:mysql://157.253.236.120:8080/WWImportersTransactional'\n",
    "\n",
    "dest_db_connection_string = 'jdbc:mysql://157.253.236.120:8080/Estudiante_19_202413'\n",
    "\n",
    "# Driver de conexion\n",
    "path_jar_driver = 'C:\\Program Files (x86)\\MySQL\\Connector J 8.0\\mysql-connector-java-8.0.28.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pyspark.sql import functions as f, SparkSession, types as t\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import udf, col, length, isnan, when, count, regexp_replace\n",
    "import mysql.connector\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\estudiante\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\pyspark\\sql\\context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "#Configuración de la sesión\n",
    "conf=SparkConf() \\\n",
    "    .set('spark.driver.extraClassPath', path_jar_driver)\n",
    "\n",
    "spark_context = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(spark_context)\n",
    "spark = sql_context.sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión y carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define la función para conexión y cargue de dataframes desde la base de datos origen y luego la función para guardar un dataframe en una tabla de la base de datos destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obterner_dataframe_desde_csv(_PATH, _sep):\n",
    "    return spark.read.load(_PATH, format=\"csv\", sep=_sep, inferSchema=\"true\", header='true')\n",
    "\n",
    "def obtener_dataframe_de_bd(db_connection_string, sql, db_user, db_psswd):\n",
    "    df_bd = spark.read.format('jdbc')\\\n",
    "        .option('url', db_connection_string) \\\n",
    "        .option('dbtable', sql) \\\n",
    "        .option('user', db_user) \\\n",
    "        .option('password', db_psswd) \\\n",
    "        .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "        .load()\n",
    "    return df_bd\n",
    "\n",
    "def guardar_db(db_connection_string, df, tabla, db_user, db_psswd):\n",
    "    df.select('*').write.format('jdbc') \\\n",
    "      .mode('append') \\\n",
    "      .option('url', db_connection_string) \\\n",
    "      .option('dbtable', tabla) \\\n",
    "      .option('user', db_user) \\\n",
    "      .option('password', db_psswd) \\\n",
    "      .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BLOQUE 1: Dimensión <i>Empleado</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------+\n",
      "|ID_Empleado|            Nombre|EsVendedor|\n",
      "+-----------+------------------+----------+\n",
      "|          2|    Kayla Woodcock|      true|\n",
      "|          3|     Hudson Onslow|      true|\n",
      "|          6|     Sophia Hinton|      true|\n",
      "|          7|         Amy Trefl|      true|\n",
      "|          8|    Anthony Grosse|      true|\n",
      "|         13|Hudson Hollinworth|      true|\n",
      "|         14|         Lily Code|      true|\n",
      "|         15|         Taj Shand|      true|\n",
      "|         16|     Archer Lamble|      true|\n",
      "|         20|       Jack Potter|      true|\n",
      "+-----------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_empleados = '''(SELECT ID_persona AS ID_Empleado, NombreCompleto AS Nombre, EsVendedor FROM WWImportersTransactional.Personas WHERE EsVendedor=1) AS Temp_empleados'''\n",
    "empleados = obtener_dataframe_de_bd(source_db_connection_string, sql_empleados, db_user, db_psswd)\n",
    "empleados.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+---------------+\n",
      "|ID_Empleado_T|        Nombre|ID_Empleado_DWH|\n",
      "+-------------+--------------+---------------+\n",
      "|            2|Kayla Woodcock|              1|\n",
      "|            3| Hudson Onslow|              2|\n",
      "|            6| Sophia Hinton|              3|\n",
      "|            7|     Amy Trefl|              4|\n",
      "|            8|Anthony Grosse|              5|\n",
      "+-------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "empleados = empleados.selectExpr('ID_Empleado as ID_Empleado_T','Nombre')\n",
    "empleados = empleados.withColumn('ID_Empleado_DWH', f.monotonically_increasing_id() + 1)\n",
    "empleados.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, empleados,'Estudiante_19_202413.Empleado', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BLOQUE 2: Dimensión ciudad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_ciudad_T', 'NombreCiudad', 'ID_EstadoProvincia', 'Poblacion'] ['ID_Pais', 'Nombre', 'Continente', 'Region', 'Subregion'] ['ID_EstadoProvincia', 'NombreEstadoProvincia', 'TerritorioVentas', 'ID_Pais']\n"
     ]
    }
   ],
   "source": [
    "#EXTRACCION\n",
    "sql_paises = '''(SELECT ID_Pais, Nombre, Continente, Region, Subregion FROM WWImportersTransactional.Paises) AS Temp_paises'''\n",
    "sql_provincias_estados = '''(SELECT ID_EstadosProvincias AS ID_EstadoProvincia, NombreEstadoProvincia, TerritorioVentas, ID_Pais FROM WWImportersTransactional.EstadosProvincias) AS Temp_estados_provincias'''\n",
    "sql_ciudades = '''(SELECT ID_ciudad as ID_ciudad_T, NombreCiudad, ID_EstadoProvincia, Poblacion FROM WWImportersTransactional.Ciudades) AS Temp_ciudades'''\n",
    "\n",
    "paises = obtener_dataframe_de_bd(source_db_connection_string, sql_paises, db_user, db_psswd)\n",
    "provincias_estados = obtener_dataframe_de_bd(source_db_connection_string, sql_provincias_estados, db_user, db_psswd)\n",
    "ciudades = obtener_dataframe_de_bd(source_db_connection_string, sql_ciudades, db_user, db_psswd)\n",
    "\n",
    "print(ciudades.columns, paises.columns, provincias_estados.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------+------------+---------+---------------------+----------------+-------------+-------------+--------+----------------+-------------+\n",
      "|ID_Pais|ID_EstadoProvincia|ID_ciudad_T|NombreCiudad|Poblacion|NombreEstadoProvincia|TerritorioVentas|       Nombre|   Continente|  Region|       Subregion|ID_Ciudad_DWH|\n",
      "+-------+------------------+-----------+------------+---------+---------------------+----------------+-------------+-------------+--------+----------------+-------------+\n",
      "|    230|                31|         49|     Absecon|     8411|           New Jersey|         Mideast|United States|North America|Americas|Northern America|            1|\n",
      "|    230|                31|        150|    Adelphia|     null|           New Jersey|         Mideast|United States|North America|Americas|Northern America|            2|\n",
      "|    230|                31|        336|      Albion|     null|           New Jersey|         Mideast|United States|North America|Americas|Northern America|            3|\n",
      "|    230|                31|        458|   Allamuchy|       78|           New Jersey|         Mideast|United States|North America|Americas|Northern America|            4|\n",
      "|    230|                31|        480|   Allendale|     6505|           New Jersey|         Mideast|United States|North America|Americas|Northern America|            5|\n",
      "+-------+------------------+-----------+------------+---------+---------------------+----------------+-------------+-------------+--------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "ciudades = ciudades.join(provincias_estados, how = 'inner', on = 'ID_EstadoProvincia')\n",
    "ciudades = ciudades.join(paises, how = 'inner', on = 'ID_Pais')\n",
    "ciudades = ciudades.withColumn('ID_Ciudad_DWH', f.monotonically_increasing_id() + 1)\n",
    "ciudades.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, ciudades,'Estudiante_19_202413.Ciudad', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLOQUE 3: Dimensión paquete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACCION\n",
    "sql_paquetes = '''(SELECT ID_TipoPaquete AS ID_TipoPaquete_T, TipoPaquete AS Nombre FROM WWImportersTransactional.Paquetes) AS Temp_Paquetes'''\n",
    "\n",
    "paquetes = obtener_dataframe_de_bd(source_db_connection_string, sql_paquetes, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------------------+\n",
      "|ID_TipoPaquete_T|Nombre|ID_TipoPaquete_DWH|\n",
      "+----------------+------+------------------+\n",
      "|               1|   Bag|                 1|\n",
      "|               2| Block|                 2|\n",
      "|               3|Bottle|                 3|\n",
      "|               4|   Box|                 4|\n",
      "|               5|   Can|                 5|\n",
      "+----------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paquetes = paquetes.withColumn('ID_TipoPaquete_DWH', f.monotonically_increasing_id() + 1)\n",
    "paquetes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, paquetes,'Estudiante_19_202413.TipoPaquete', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE 4: Dimensión producto\n",
    "En términos generales para no afectar la carga realizada en el proceso ETL anterior, vamos a crear una copia de la dimensión Producto y la llamaremos ProductoHistoria. Es esta última en la que realizaremos la nueva carga de datos. En un contexto real lo que se hace es modificar la misma dimensión para manejar la historia y realizar el cargue incremental.\n",
    "\n",
    "En detalle lo que vamos a hacer es:\n",
    "1. Paso 1: Crear y cargar la dimensión ProductoHistoria a partir de los datos de la dimensión Producto agregando las columnas para el manejo de historia tipo 2.\n",
    "2. Paso 2: Ingresar y actualizar los registros que lo requieran en la dimensión ProductoHistoria a partir de la información del  archivo ReporteNum2_Productos.csv. Este reporte lo entrega el negocio a corte 2015-01-01\n",
    "\n",
    "Nota: El paso 1 incluye la creación de la tabla y la carga a partir de la información existente en la dimensión Producto. Este proceso si se hace en PySpark se puede realizar en un solo paso, al guardar un dataframe ya que este proceso incluye la creación de  las tablas en la base de datos.\n",
    "\n",
    "Para las restricciones de llaves primarias le recomendamos usar MySQL Workbench, dando clic derecho sobre el nombre de la tabla, clic en <i>Alter table ...</i>, seleccionando el(los) atributo(s) que es(son) la llave primaria y dando clic en <i>Apply</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1: Crear y cargar la Dimensión ProductoHistoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_productos = '''(SELECT * FROM WWImportersTransactional.Producto) AS Temp_productos'''\n",
    "productos = obtener_dataframe_de_bd(source_db_connection_string, sql_productos, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+--------+----------------------+-------------------+--------+--------------+-----------------+------------+----------+-------+---------------+\n",
      "|ID_Producto_T|      NombreProducto|Marca|ID_Color|Necesita_refrigeracion|Dias_tiempo_entrega|Impuesto|PrecioUnitario|PrecioRecomendado|FechaInicial|FechaFinal|Version|ID_Producto_DWH|\n",
      "+-------------+--------------------+-----+--------+----------------------+-------------------+--------+--------------+-----------------+------------+----------+-------+---------------+\n",
      "|            1|USB missile launc...| null|    null|                     0|                 14|      15|            25|               37|  2013-01-01|2199-12-31|      1|              1|\n",
      "|            2|USB rocket launch...| null|      12|                     0|                 14|      15|            25|               37|  2013-01-01|2199-12-31|      1|              2|\n",
      "|            3|Office cube peris...| null|       3|                     0|                 14|      15|            19|               28|  2013-01-01|2199-12-31|      1|              3|\n",
      "|            4|USB food flash dr...| null|    null|                     0|                 14|      15|            32|               48|  2013-01-01|2199-12-31|      1|              4|\n",
      "|            5|USB food flash dr...| null|    null|                     0|                 14|      15|            32|               48|  2013-01-01|2199-12-31|      1|              5|\n",
      "+-------------+--------------------+-----+--------+----------------------+-------------------+--------+--------------+-----------------+------------+----------+-------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productosHistoria = productos.withColumn('FechaInicial',lit('2013-01-01'))\n",
    "productosHistoria = productosHistoria.withColumn('FechaFinal',lit('2199-12-31'))\n",
    "productosHistoria = productosHistoria.withColumn('Version', lit(1))\n",
    "productosHistoria = productosHistoria.withColumnRenamed(\"ID_Producto\",\"ID_Producto_T\")\n",
    "productosHistoria = productosHistoria.withColumn('ID_Producto_DWH', f.monotonically_increasing_id() + 1)\n",
    "productosHistoria.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga inicial de la dimensión ProductoHistoria\n",
    "El cargue inicial solo se realiza una vez, mientras que el incremental se realiza en cada reporte de negocio de datos en un periodo de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, productosHistoria,'Estudiante_19_202413.ProductoHistoria', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2: Manejo de historia\n",
    "Los reportes del negocio pueden ser varios a lo largo del tiempo, por ende el cargue incremental se hace con cada reporte que nos entrega el negocio. En cada reporte las versiones de los registros aumentan, por lo que podemos encontrar registros con versión 2, versión 3, versión 4 hasta versión N, siendo N el número de reporte. A continuación vamos a simular el primer reporte dado por el negocio despues del primer cargue, entonces todos los productos que existen en la tabla ProductoHistoria son la primera (y única) versión de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 227)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "productosReporte = obterner_dataframe_desde_csv('.\\ReporteNum2_Productos.csv', ',')\n",
    "sql_productosHistoria ='''(SELECT * FROM Estudiante_19_202413.ProductoHistoria) AS Temp_productos'''\n",
    "\n",
    "productosHistoria = obtener_dataframe_de_bd(source_db_connection_string, sql_productosHistoria, db_user, db_psswd)\n",
    "productosReporte.count(), productosHistoria.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la bodega de datos (base de datos Estudiante_i), se tienen varias versiones para un mismo Producto en un proceso incremental (N reportes). Sin embargo, para el manejo de historia tipo 2 sólo se necesita la última versión, se utiliza una función Window de PySpark para extraer únicamente la última versión por cada llave primaria de un Producto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se utiliza un Window para extraer del dataframe de Productos, por cada ID_Producto, la versión mayor.\n",
    "#Es decir, por cada llave natural, extrae la última versión.\n",
    "window = Window.partitionBy(productosHistoria['ID_producto_T']).orderBy(productosHistoria['Version'].desc())\n",
    "productosHistoria = productosHistoria.select('*', f.rank().over(window).alias('rank')).filter(f.col('rank') == 1) \n",
    "productosHistoria = productosHistoria.drop('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación\n",
    "hacemos un LEFT JOIN para unir los productos del reporte y los productos de la tabla ProductoHistoria, de manera que se mantienen todos los registros del DataFrame con historia (La primera vez tiene la información cargada en el primer cargue y por ende todas las versiones tienen el valor de 1, FechaInicial 2013-01-01 y FechaFinal 2199-12-31)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renombra columnas del DataWarehouse, poniendo sujifo _DWH para evitar confusiones\n",
    "productosHistoria = productosHistoria.selectExpr('ID_Producto_T','ID_Producto_DWH','Version as Version_DWH', 'PrecioUnitario as PrecioUnitario_DWH', 'FechaInicial as FechaInicial_DWH', 'FechaFinal as FechaFinal_DWH')\n",
    "\n",
    "#Realiza left join entre productosReporte que se están procesando y productosHistoria en el DW.\n",
    "productosHistoria = productosReporte.withColumnRenamed('ID_Producto', 'ID_Producto_T').join(productosHistoria, how = 'left', on ='ID_Producto_T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar una nueva variable \"ETIQUETA\" que nos indica el tipo de registro para los productos del reporte:\n",
    "- **-1** que representa \"cambios\": un registro que ya existe en ProductoHistoria y que cambio el valor de precio unitario\n",
    "- **0** que representa \"sin cambios\": un registro que ya existe en ProductoHistoria y que no cambio su precio unitario\n",
    "- **1** que representa \"nuevo\": un registro que no existia en ProductoHistoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "productosHistoria = productosHistoria.withColumn('ETIQUETA', \n",
    "    f.when(productosHistoria['ID_Producto_DWH'].isNull(), 1)\\\n",
    "    .when((productosHistoria['ID_Producto_DWH'].isNotNull()) \n",
    "          & ((productosHistoria['PrecioUnitario'] == productosHistoria['PrecioUnitario_DWH']) \n",
    "              | (productosHistoria['PrecioUnitario'].isNull() & productosHistoria['PrecioUnitario_DWH'].isNull())\n",
    "          ), 0)\\\n",
    "    .otherwise(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los registros que no han cambiado (ETIQUETA = 0), no deben mantenerse en el DataFrame, por lo que se excluyen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "productosHistoria = productosHistoria.where(productosHistoria['ETIQUETA'] != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario insertar nuevos registros al Data Warehouse y, por lo tanto, estos necesitan un nuevo id (ID_Producto_DWH). Para crear el id, primero se encuentra el id máximo de los productos existentes y, paso seguido, se utiliza el máximo junto con la función de PySpark monotonically_increasing_id para crear un nuevo id único para cada uno de los registros del reporte. Este paso también se puede realizar usando secuencias en MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontar llave máxima\n",
    "max_key = productosHistoria.agg({\"ID_Producto_DWH\": \"max\"}).collect()[0][0]\n",
    "    \n",
    "if max_key is None:\n",
    "    max_key = 1\n",
    "    \n",
    "#Nuevos ids\n",
    "productosHistoria = productosHistoria.withColumn('new_id', f.monotonically_increasing_id() + max_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, se procede a asignar/cambiar la versión, fecha de inicio y fecha de vencimiento o final a los registros de acuerdo con las categorías previamente determinadas. En este punto ETIQUETA solo puede tomar los valores de 1 y -1. Asignamos a una variable <i>fecha_reporte</i> la fecha del reporte, pues es la fecha en la que las versiones hasta ahora vigentes dejan de estarlo y entran en vigencia las nuevas versiones con los últimos cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_reporte = '2015-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Caso de ETIQUETA=1: Productos nuevos\n",
    "\n",
    "- **Nota 1**: Se asigna Version igual a 1, fecha inicial igual a la fecha de reporte (2015-01-01), fecha final igual a 2199-12-31\n",
    "\n",
    "- **Nota 2**: Note que en la tercera línea de código (FechaFinal_DWH) si la ETIQUETA no es 1, entra al <i>otherwise</i>, en donde actualizamos la fecha final de vigencia de los registros que cambiaron (**caso ETIQUETA=-1**) como la fecha de reporte menos un día, es decir que dejan de ser vigentes hasta el día anterior al reporte. Note que los registros que cambiaron se van a agregar (**En Caso de ETIQUETA=-1: Productos que cambiaron**) como nuevos registros a la tabla con una versión posterior a la que existia, la fecha inicial como la del reporte y la final como 2199-12-31, es decir los productos que cambiaron se van a insertar como una nueva versión de los productos que ya existian.\n",
    "\n",
    "- **Nota 3**: En la última línea de código estamos creando una segunda variable indicadora A_INSERTAR que toma el valor de 1 para registros que se van a INSERTAR directamente en la tabla ProductoHistoria y 0 para los que se van a actualizar (los productos que dejan de estar en vigencia). En el caso ETIQUETA=1, estamos diciendo que todos los productos nuevos se van a insertar directamente, mientras que, por ahora, los que cambiaron se van a actualizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso 1: Registros nuevos se asigna version igual a 1, fecha inicial la actual y fecha final 2199-12-31\n",
    "# Note que aqui no cambiamos los valores en el otherwise dado que perderiamos la historia al sobreescribir la información con la de los registros que cambiaron\n",
    "productosHistoria = productosHistoria.withColumn('Version_DWH', f.when(productosHistoria['ETIQUETA'] == 1, 1).otherwise(productosHistoria['Version_DWH']))\n",
    "productosHistoria = productosHistoria.withColumn('FechaInicial_DWH', f.when(productosHistoria['ETIQUETA'] == 1, f.to_date(f.lit(fecha_reporte), 'yyyy-MM-dd')).otherwise(productosHistoria['FechaInicial_DWH']))\n",
    "# En el otherwise estamos actualizando la fecha final del Caso -1: si son registros que ya existian y cambiaron, su fecha final será la fecha actual menos un dia\n",
    "productosHistoria = productosHistoria.withColumn('FechaFinal_DWH', f.when(productosHistoria['ETIQUETA'] == 1, f.to_date(f.lit('2199-12-31'), 'yyyy-MM-dd')).otherwise(f.to_date(f.lit(fecha_reporte), 'yyyy-MM-dd')-1))\n",
    "productosHistoria = productosHistoria.withColumn('A_INSERTAR', f.when(productosHistoria['ETIQUETA'] == 1, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Caso de ETIQUETA=-1: Productos que cambiaron\n",
    "\n",
    "- **Nota 4**: Para los productos que cambiaron tenemos dos subcasos: el registro con el cambio que va a ser una nueva versión y que no existe en la base de datos y los registros de esos productos que cambiaron que si existian en la base de datos que dejan de estar vigentes, en cuyo caso hay que actualizar la FechaFinal\n",
    "\n",
    "- **Nota 5**: Dada la nota 4, lo primero que hacemos es separar los registros que cambiaron y almacenarlos en un dataframe diferente, es decir aquellos con ETIQUETA=-1. Luego asignamos como versión nueva el valor de la versión anterior más 1; como fecha inicial la fecha de reporte de los datos, como fecha final 2199-12-31 \n",
    "\n",
    "- **Nota 6**: En este caso la variable A_INSERTAR toma el valor de 1, es decir todos estos registros que son las nuevas versiones de productos que ya existian, se van a insertar directamente en la tabla ProductoHistoria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caso -1: Los registros ya existian pero cambiaron: no se van a insertar, se van a actualizar\n",
    "# Separamos los productos que cambiaron, creando una copia de los productos cuya ETIQUETA=-1\n",
    "productos_nuevas_versiones = productosHistoria.where(productosHistoria['ETIQUETA'] == -1)\n",
    "# Asignamos a estos registros una versión igual a la última+1, fecha inicial de vigencia del registro: la fecha actual, y como fecha final 2199-12-31\n",
    "productos_nuevas_versiones = productos_nuevas_versiones.withColumn('Version_DWH', productos_nuevas_versiones['Version_DWH'] + 1)\n",
    "productos_nuevas_versiones = productos_nuevas_versiones.withColumn('FechaInicial_DWH', f.to_date(f.lit(fecha_reporte), 'yyyy-MM-dd'))\n",
    "productos_nuevas_versiones = productos_nuevas_versiones.withColumn('FechaFinal_DWH', f.to_date(f.lit('2199-12-31'), 'yyyy-MM-dd'))\n",
    "productos_nuevas_versiones = productos_nuevas_versiones.withColumn('A_INSERTAR',f.lit(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, unimos los dos dataframes, de manera que la variable A_INSERTAR toma dos valores: 1 o 0. 1 son los registros que se van a insertar directamente, estos son los productos nuevos y las versiones nuevas de los productos que ya existian pero cambiaron. 0 son los registros de las últimas versiones que habian en la base de datos de los productos que cambiaron cuya FechaFinal se va a actualizar pues dejan de estar en vigencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir los DataFrames con los registros nuevos, los registros de versiones anteriores y los registros que son nuevas versiones de registros existentes\n",
    "productosHistoria = productosHistoria.union(productos_nuevas_versiones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para hacer la actualización y posteriormente la inserción se divide el DataFrame en dos, una parte conteniendo los registros a actualizarse y otra los registros a insertar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persiste DF para obligar ejecución de grafo de computación. \n",
    "# Este persist es independiente de la base de datos, es más sobre pasar de memoria a disco el dataframe\n",
    "productosHistoria.persist()\n",
    "productosHistoria = productosHistoria.selectExpr('new_id as ID_Producto_DWH',  'NombreProducto', 'Necesita_refrigeracion', 'Dias_tiempo_entrega', 'Impuesto', \\\n",
    "                                 'PrecioUnitario', 'PrecioRecomendado','ID_Color', 'Marca', 'ID_PRODUCTO_T', 'Version_DWH as Version',\\\n",
    "                                 'FechaInicial_DWH as FechaInicial', 'FechaFinal_DWH as FechaFinal', 'A_INSERTAR')\n",
    "\n",
    "#Separar en registros que se deben insertar y registros que se deben actualizar.\n",
    "productos_inserts = productosHistoria.where(productosHistoria['A_INSERTAR'] == 1)\n",
    "productos_updates = productosHistoria.where(productosHistoria['A_INSERTAR'] == 0)\n",
    "\n",
    "productos_inserts = productos_inserts.drop('A_INSERTAR')\n",
    "productos_updates = productos_updates.drop('A_INSERTAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga incremental de la dimensión ProductoHistoria\n",
    "Primero hacemos la **actualización**, dado que hay menos registros en la tabla, la búsqueda y actualización es menor comparado a si lo hicieramos después de insertar los productos nuevos. La actualización es un poco más compleja que la inserción directa, puesto que Spark no lo soporta. Por lo tanto, el proceso se realiza en la base de datos, para lo cual se procede a insertar los registros en una tabla auxiliar (ProductoHistoria_Update), se ejecuta una sentencia SQL UPDATE para actualizar la tabla de la dimensión ProductoHistoria a partir de esta tabla auxiliar, cuyos registros son borrados al final con una sentencia TRUNCATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertar en tabla auxiliar (ProductoHistoria_Update)\n",
    "guardar_db(dest_db_connection_string, productos_updates,'Estudiante_19_202413.ProductoHistoria_Update', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x000001DB2CE7D748>\n"
     ]
    }
   ],
   "source": [
    "#Ejecutar sentencia SQL UPDATE\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"157.253.236.120\",\n",
    "  port=\"8080\",\n",
    "  user=\"Estudiante_19_202413\",\n",
    "  passwd=\"aabb1122\",\n",
    "  database=\"Estudiante_19_202413\",\n",
    ")\n",
    "cur = mydb.cursor()\n",
    "\n",
    "print(mydb)\n",
    "res = cur.execute(\"\"\"\n",
    "    UPDATE ProductoHistoria \n",
    "    INNER JOIN ProductoHistoria_Update AS Table_Updates\n",
    "        ON ProductoHistoria.ID_Producto_T = Table_Updates.ID_Producto_T\n",
    "    SET ProductoHistoria.FechaFinal = Table_Updates.FechaFinal;\n",
    "\"\"\")\n",
    "res = cur.execute(\"\"\"TRUNCATE TABLE ProductoHistoria_Update;\"\"\")\n",
    "mydb.commit()\n",
    "cur.close()\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, hacemos la **inserción** de productos nuevos y de las nuevas versiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_db(dest_db_connection_string, productos_inserts,'Estudiante_19_202413.ProductoHistoria', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLOQUE 5: Dimensión cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_categoriasCliente = '''(SELECT ID_Categoria, NombreCategoria FROM WWImportersTransactional.CategoriasCliente) AS Temp_categoriasclientes'''\n",
    "sql_gruposCompra = '''(SELECT ID_GrupoCompra, NombreGrupoCompra FROM WWImportersTransactional.GruposCompra) AS Temp_gruposcompra'''\n",
    "sql_clientes = '''(SELECT ID_Cliente AS ID_Cliente_T, Nombre, ClienteFactura, ID_Categoria, ID_GrupoCompra, ID_CiudadEntrega, LimiteCredito, FechaAperturaCuenta, DiasPago FROM WWImportersTransactional.Clientes) AS Temp_clientes'''\n",
    "\n",
    "categoriasCliente = obtener_dataframe_de_bd(source_db_connection_string, sql_categoriasCliente, db_user, db_psswd)\n",
    "gruposCompra = obtener_dataframe_de_bd(source_db_connection_string, sql_gruposCompra, db_user, db_psswd)\n",
    "clientes = obtener_dataframe_de_bd(source_db_connection_string, sql_clientes, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------+--------------------+--------------+----------------+-------------+-------------------+--------+-----------------+---------------+--------------+\n",
      "|ID_Categoria|ID_GrupoCompra|ID_Cliente_T|              Nombre|ClienteFactura|ID_CiudadEntrega|LimiteCredito|FechaAperturaCuenta|DiasPago|NombreGrupoCompra|NombreCategoria|ID_Cliente_DWH|\n",
      "+------------+--------------+------------+--------------------+--------------+----------------+-------------+-------------------+--------+-----------------+---------------+--------------+\n",
      "|           3|             2|         601|Wingtip Toys (Rut...|           401|           29887|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|             1|\n",
      "|           3|             2|         600|Wingtip Toys (Car...|           401|            5407|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|             2|\n",
      "|           3|             2|         599|Wingtip Toys (Dic...|           401|            9077|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|             3|\n",
      "|           3|             2|         598|Wingtip Toys (Kap...|           401|           17340|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|             4|\n",
      "|           3|             2|         597|Wingtip Toys (Hay...|           401|           14965|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|             5|\n",
      "|           3|             2|         596|Wingtip Toys (Cos...|           401|            7695|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|             6|\n",
      "|           3|             2|         595|Wingtip Toys (Acc...|           401|              54|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|             7|\n",
      "|           3|             2|         594|Wingtip Toys (Mar...|           401|           20924|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|             8|\n",
      "|           3|             2|         593|Wingtip Toys (Cuy...|           401|            8296|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|             9|\n",
      "|           3|             2|         592|Wingtip Toys (Dun...|           401|            9549|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            10|\n",
      "|           3|             2|         591|Wingtip Toys (Ida...|           401|           16382|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            11|\n",
      "|           3|             2|         590|Wingtip Toys (McA...|           401|           21405|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            12|\n",
      "|           3|             2|         589|Wingtip Toys (Alc...|           401|             357|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            13|\n",
      "|           3|             2|         588|Wingtip Toys (Oak...|           401|           24988|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            14|\n",
      "|           3|             2|         587|Wingtip Toys (Pla...|           401|           27236|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            15|\n",
      "|           3|             2|         586|Wingtip Toys (Lyn...|           401|           20376|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            16|\n",
      "|           3|             2|         585|Wingtip Toys (Tow...|           401|           34285|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            17|\n",
      "|           3|             2|         584|Wingtip Toys (Cac...|           401|            4840|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            18|\n",
      "|           3|             2|         583|Wingtip Toys (Kni...|           401|           17935|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            19|\n",
      "|           3|             2|         582|Wingtip Toys (Mon...|           401|           22814|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|            20|\n",
      "+------------+--------------+------------+--------------------+--------------+----------------+-------------+-------------------+--------+-----------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "clientes = clientes.join(gruposCompra, how = 'inner', on = 'ID_GrupoCompra')\n",
    "clientes = clientes.join(categoriasCliente, how = 'inner', on = 'ID_Categoria')\n",
    "clientes = clientes.withColumn('ID_Cliente_DWH', f.monotonically_increasing_id() + 1)\n",
    "clientes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column \"ID_Categoria\" not found in schema Some(StructType(StructField(ID_Cliente_DWH,IntegerType,false), StructField(ID_Cliente_T,IntegerType,true), StructField(Nombre,StringType,true), StructField(ClienteFactura,StringType,true), StructField(ID_CiudadEntrega_DWH,IntegerType,true), StructField(LimiteCredito,DecimalType(10,2),true), StructField(FechaAperturaCuenta,DateType,true), StructField(DiasPago,IntegerType,true), StructField(NombreGrupoCompra,StringType,true), StructField(NombreCategoria,StringType,true)))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6928\\3292895931.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# CARGUE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mguardar_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdest_db_connection_string\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclientes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Estudiante_19_202413.Cliente'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_user\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_psswd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6928\\211298303.py\u001b[0m in \u001b[0;36mguardar_db\u001b[1;34m(db_connection_string, df, tabla, db_user, db_psswd)\u001b[0m\n\u001b[0;32m     19\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_user\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'password'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_psswd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m       \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'driver'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'com.mysql.cj.jdbc.Driver'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column \"ID_Categoria\" not found in schema Some(StructType(StructField(ID_Cliente_DWH,IntegerType,false), StructField(ID_Cliente_T,IntegerType,true), StructField(Nombre,StringType,true), StructField(ClienteFactura,StringType,true), StructField(ID_CiudadEntrega_DWH,IntegerType,true), StructField(LimiteCredito,DecimalType(10,2),true), StructField(FechaAperturaCuenta,DateType,true), StructField(DiasPago,IntegerType,true), StructField(NombreGrupoCompra,StringType,true), StructField(NombreCategoria,StringType,true)))"
     ]
    }
   ],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string,clientes,'Estudiante_19_202413.Cliente', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLOQUE 6: Hecho orden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_ordenes_tmp = '''(SELECT * FROM WWImportersTransactional.OrdenesCopia) AS Temp_ordenes'''\n",
    "sql_detallesOrdenes = '''(SELECT * FROM WWImportersTransactional.DetallesOrdenesCopia) AS Temp_detallesordenes'''\n",
    "ordenes_tmp = obtener_dataframe_de_bd(source_db_connection_string, sql_ordenes_tmp, db_user, db_psswd)\n",
    "detallesOrdenes = obtener_dataframe_de_bd(source_db_connection_string, sql_detallesOrdenes, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación\n",
    "Estas son las respuestas de Wide World Importers a los conclusiones obtenidas en el entendimiento de los datos:\n",
    "- La regla de negocio \"La tasa de impuesto es de 10% o 15%\" es correcta, pero habian errores en la tabla original, que fueron corregidos. \n",
    "- Para la segunda regla de negocio: \"Son 73.595 órdenes detalladas en 231.412 lineas de detalle de órdenes realizadas desde 2013\", si faltaban datos, los cuales fueron completados, y nos dicen que en cuanto a consistencia ellos revisaron las tablas e hicieron correcciones, pero que los duplicados completos de ordenes los eliminemos\n",
    "- \"El formato de fechas manejado es YYYY-MM-DD HH:MM:SS si tienen hora, minutos y segundos. De lo contrario el formato es YYYY-MM-DD\": En cuanto a formatos de fechas estan de acuerdo con que los estandarizemos y el formato sea el especificado en la regla\n",
    "- Para las descripciones de productos que eran \"a\", se actualizaron a los valores reales. \n",
    "- Se pueden eliminar las columnas Comenarios, Instrucciones_de_entrega y comentarios_internos porque estan vacias. \n",
    "- A pesar de estar en un proceso de mejorar la calidad de los datos y mantener los nulos nos ayudaría a reflejar esa calidad, de la mano con el grupo de analitica de WWI se decide imputar por la media el valor extremo de la variable Cantidad\n",
    "- Para las ordenes las columnas Seleccionado_por_ID_de_persona, ID_de_pedido_pendiente, Seleccion_completada_cuando, y para las columnas Seleccion_completada_cuando de la tabla detalles de ordenes, se decide mantener los valores vacíos, sin embargo para la variable Precio_unitario el negocio reviso y complemento los valores faltantes\n",
    "\n",
    "Las tablas usadas en el tutorial de entendimiento de datos estaran disponibles para su revision con los siguientes nombres: OrdenesCopia y DetallesOrdenesCopia. \n",
    "\n",
    "Para este tutorial vamos a trabajar con unas tablas que dadas las conclusiones del tutorial de entendimiento, WWImporters revisó los datos originales, creo tablas y las llamo \"Ordenes\" y \"DetallesOrdenes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace una verificación de los valores de la tasa de impuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Tasa_de_impuesto|\n",
      "+----------------+\n",
      "|              15|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detallesOrdenes.select(\"Tasa_de_impuesto\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace una verificación del rango de fechas disponible en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|min(Fecha_de_pedido)|\n",
      "+--------------------+\n",
      "|          2014-01-01|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordenes_tmp.agg({\"Fecha_de_pedido\": \"min\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columnas Comenarios, Instrucciones_de_entrega y comentarios_internos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes_tmp = ordenes_tmp.drop(*[\"Comentarios\", \"Instrucciones_de_entrega\",\"comentarios_internos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan duplicados totales de ordenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88258, 74179)\n"
     ]
    }
   ],
   "source": [
    "print((ordenes_tmp.count(),ordenes_tmp.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes_tmp = ordenes_tmp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74179, 74179)\n"
     ]
    }
   ],
   "source": [
    "print((ordenes_tmp.count(),ordenes_tmp.distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace verificación de consistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44359, 28956)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#consistencia: revisar genially: definicion de consistencia\n",
    "ids_ordenes = set([x.ID_de_pedido for x in ordenes_tmp.select('ID_de_pedido').collect()])\n",
    "ids_detalles = set([x.ID_de_pedido for x in detallesOrdenes.select('ID_de_pedido').collect()])\n",
    "\n",
    "len(ids_ordenes-ids_detalles), len(ids_detalles-ids_ordenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código para el manejo de fechas, pasamos del formato MM dd,YYYY al formato establecido en la regla de negocio<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20034 54145\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "|ID_de_pedido|ID_de_cliente|ID_de_vendedor|Seleccionado_por_ID_de_persona|ID_de_persona_de_contacto|ID_de_pedido_pendiente|Fecha_de_pedido|Fecha_de_entrega_esperada|Numero_de_pedido_de_compra_del_cliente|Pedido_pendiente_de_suministro_insuficiente|Seleccion_completada_cuando|\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "|       62796|           69|            20|                            18|                     1137|                  null|    Dec 11,2015|               2015-12-14|                                 18307|                                       true|        2015-12-11 11:00:00|\n",
      "|       48788|           23|             7|                            13|                     1045|                  null|    May 09,2015|               2015-05-11|                                 18413|                                       true|        2015-05-13 11:00:00|\n",
      "|       49115|          982|            20|                             5|                     3182|                  null|    May 15,2015|               2015-05-18|                                 16448|                                       true|        2015-05-15 11:00:00|\n",
      "|       43417|          475|            16|                            16|                     2149|                  null|    Feb 17,2015|               2015-02-18|                                 10215|                                       true|        2015-02-17 11:00:00|\n",
      "|       45484|         1032|             2|                            19|                     3232|                  null|    Mar 23,2015|               2015-03-24|                                 19093|                                       true|        2015-03-23 11:00:00|\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "regex = \"([0-2]\\d{3}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1]))\"\n",
    "cumplenFormato = ordenes_tmp.filter(ordenes_tmp[\"Fecha_de_pedido\"].rlike(regex))\n",
    "noCumplenFormato = ordenes_tmp.filter(~ordenes_tmp[\"Fecha_de_pedido\"].rlike(regex))\n",
    "print(noCumplenFormato.count(), cumplenFormato.count())\n",
    "print(noCumplenFormato.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20034, 74179)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "# En este punto tomamos los registros que tiene los formatos de fecha Feb 20,2014 y lo transformamos al formato estandar dado por WWImporters.\n",
    "noCumplenFormato = noCumplenFormato.withColumn('Fecha_de_pedido', f.udf(lambda d: datetime.strptime(d, '%b %d,%Y').strftime('%Y-%m-%d'), t.StringType())(f.col('Fecha_de_pedido')))\n",
    "ordenes_tmp = noCumplenFormato.union(cumplenFormato)\n",
    "noCumplenFormato.count(), ordenes_tmp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "|ID_de_pedido|ID_de_cliente|ID_de_vendedor|Seleccionado_por_ID_de_persona|ID_de_persona_de_contacto|ID_de_pedido_pendiente|Fecha_de_pedido|Fecha_de_entrega_esperada|Numero_de_pedido_de_compra_del_cliente|Pedido_pendiente_de_suministro_insuficiente|Seleccion_completada_cuando|\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "|       20972|          132|             6|                             8|                     1263|                  null|     2014-01-28|               2014-01-29|                                 14875|                                       true|        2014-01-28 11:00:00|\n",
      "|       22080|          107|            16|                            12|                     1213|                  null|     2014-02-17|               2014-02-18|                                 10447|                                       true|        2014-02-17 11:00:00|\n",
      "|       22194|          136|             3|                          null|                     1271|                 22217|     2014-02-19|               2014-02-20|                                 12442|                                       true|        2014-02-19 12:00:00|\n",
      "|       22298|          969|             2|                             7|                     3169|                  null|     2014-02-20|               2014-02-21|                                 11473|                                       true|        2014-07-10 11:00:00|\n",
      "|       22358|          534|             8|                             6|                     2267|                  null|     2014-02-21|               2014-02-24|                                 10078|                                       true|        2014-02-21 11:00:00|\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifricar que las fechas de no cumplen formato ya quedaron en el formato estandar.\n",
    "noCumplenFormato.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-----------+-----------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "|Detalle_orden_ID|ID_de_pedido|ID_Producto|Descripcion|ID_Tipo_Paquete|Cantidad|Precio_unitario|Tasa_de_impuesto|Cantidad_seleccionada|Seleccion_completada_cuando|\n",
      "+----------------+------------+-----------+-----------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "|          206862|       65698|         10|          a|              7|       5|             32|              15|                    5|        2016-01-27 11:00:00|\n",
      "|          206874|       65701|         47|          a|              7|       5|             13|              15|                    5|        2016-01-27 11:00:00|\n",
      "|          206909|       65711|         37|          a|              7|       5|             13|              15|                    5|        2016-01-27 11:00:00|\n",
      "|          206914|       65712|        119|          a|              7|       5|             32|              15|                    5|        2016-01-27 11:00:00|\n",
      "|          206938|       65719|          1|          a|              7|       5|             25|              15|                    5|        2016-01-27 11:00:00|\n",
      "|          206939|       65719|         62|          a|              7|       5|             25|              15|                    5|        2016-01-27 11:00:00|\n",
      "|          206942|       65720|        128|          a|              7|       5|             32|              15|                    5|        2016-01-27 11:00:00|\n",
      "|          206946|       65722|         30|          a|              7|       5|             13|              15|                    5|        2016-01-27 11:00:00|\n",
      "|          206989|       65737|        119|          a|              7|       5|             32|              15|                    5|        2016-01-28 11:00:00|\n",
      "|          207023|       65747|         15|          a|              7|       5|            240|              15|                    5|        2016-01-28 11:00:00|\n",
      "|          207033|       65750|         62|          a|              7|       5|             25|              15|                    5|        2016-01-28 11:00:00|\n",
      "|          207056|       65756|         60|          a|              7|       5|             25|              15|                    5|        2016-01-28 11:00:00|\n",
      "|          207069|       65760|         52|          a|              7|       5|             13|              15|                    5|        2016-01-28 11:00:00|\n",
      "|          207175|       65790|         44|          a|              7|       5|             13|              15|                    5|        2016-01-28 11:00:00|\n",
      "|          207178|       65791|         21|          a|              7|       5|             13|              15|                    5|        2016-01-28 11:00:00|\n",
      "|          207189|       65798|        175|          a|              7|       5|            240|              15|                    5|        2016-01-29 11:00:00|\n",
      "|          207296|       65827|        151|          a|              7|       5|             16|              15|                    5|        2016-01-29 11:00:00|\n",
      "|          207318|       65833|        213|          a|              7|       5|             87|              15|                    5|        2016-01-29 11:00:00|\n",
      "|          207320|       65834|         39|          a|              7|       5|             13|              15|                    5|        2016-01-29 11:00:00|\n",
      "|          207323|       65836|        107|          a|              7|       5|             25|              15|                    5|        2016-01-29 11:00:00|\n",
      "+----------------+------------+-----------+-----------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detallesOrdenes.where(length(col(\"Descripcion\")) <= 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputar valor maximo de cantidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Cantidad=360)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detallesOrdenes.select('Cantidad').sort(col(\"Cantidad\").desc()).collect()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "detallesOrdenes = detallesOrdenes.replace( 10000000, 360, 'Cantidad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Cantidad=360)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detallesOrdenes.select('Cantidad').sort(col(\"Cantidad\").desc()).collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se unen los dos dataframes, se verifica que no haya duplicados y si los hay se eliminan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32090, 23227)\n",
      "+--------------+-----------+---------------+-------------+--------------+---------------+--------+-----------+---------+\n",
      "|ID_de_pedido_T|ID_Producto|Fecha_de_pedido|ID_de_cliente|ID_de_vendedor|ID_Tipo_Paquete|Cantidad|Valor_total|Impuestos|\n",
      "+--------------+-----------+---------------+-------------+--------------+---------------+--------+-----------+---------+\n",
      "|         72794|        215|     2016-05-19|          939|            13|              7|       9|      17091|   256365|\n",
      "|         69493|        150|     2016-03-31|           37|            20|              7|       7|        112|     1680|\n",
      "|         68944|          5|     2016-03-23|         1055|            16|              7|       2|         64|      960|\n",
      "|         72260|         46|     2016-05-11|         1049|             2|              7|       8|        104|     1560|\n",
      "|         67020|         67|     2016-02-20|          525|             7|              7|       8|       1840|    27600|\n",
      "+--------------+-----------+---------------+-------------+--------------+---------------+--------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordenes_tmp = ordenes_tmp.join(detallesOrdenes, how = 'inner', on = 'ID_de_pedido')\n",
    "ordenes_tmp = ordenes_tmp.withColumn('Valor_total',col('Precio_unitario')*col('Cantidad'))\n",
    "ordenes_tmp = ordenes_tmp.withColumn('Impuestos',col('Valor_total')*col('Tasa_de_impuesto'))\n",
    "ordenes_tmp = ordenes_tmp.selectExpr('ID_de_pedido as ID_de_pedido_T','ID_Producto','Fecha_de_pedido','ID_de_cliente','ID_de_vendedor','ID_Tipo_Paquete','Cantidad','Valor_total', 'Impuestos')\n",
    "\n",
    "print((ordenes_tmp.count(),ordenes_tmp.distinct().count()))\n",
    "\n",
    "ordenes_tmp = ordenes_tmp.drop_duplicates()\n",
    "ordenes_tmp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga la tabla temporal relacionada con el hecho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_db(dest_db_connection_string, ordenes_tmp,'Estudiante_19_202413.Hecho_Orden_Tmp', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto la única dimensión que falta por crear es la fecha. Aquí se crea utilizando como base un archvio .csv compartido por el grupo consultor de WWImporters, y se sube al date warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o650.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 190.0 failed 1 times, most recent failure: Lost task 0.0 in stage 190.0 (TID 944) (MISW--08756.sis.virtual.uniandes.edu.co executor driver): java.sql.BatchUpdateException: Incorrect integer value: 'September' for column 'Mes' at row 1\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.SQLException: Incorrect integer value: 'September' for column 'Mes' at row 1\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:893)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.BatchUpdateException: Incorrect integer value: 'September' for column 'Mes' at row 1\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.sql.SQLException: Incorrect integer value: 'September' for column 'Mes' at row 1\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 16 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6928\\2745138635.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfecha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobterner_dataframe_desde_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.\\Fecha.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mguardar_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdest_db_connection_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfecha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Estudiante_19_202413.Fecha'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_user\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_psswd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6928\\211298303.py\u001b[0m in \u001b[0;36mguardar_db\u001b[1;34m(db_connection_string, df, tabla, db_user, db_psswd)\u001b[0m\n\u001b[0;32m     19\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_user\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'password'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_psswd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m       \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'driver'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'com.mysql.cj.jdbc.Driver'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o650.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 190.0 failed 1 times, most recent failure: Lost task 0.0 in stage 190.0 (TID 944) (MISW--08756.sis.virtual.uniandes.edu.co executor driver): java.sql.BatchUpdateException: Incorrect integer value: 'September' for column 'Mes' at row 1\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.SQLException: Incorrect integer value: 'September' for column 'Mes' at row 1\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:893)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.BatchUpdateException: Incorrect integer value: 'September' for column 'Mes' at row 1\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.sql.SQLException: Incorrect integer value: 'September' for column 'Mes' at row 1\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 16 more\r\n"
     ]
    }
   ],
   "source": [
    "fecha = obterner_dataframe_desde_csv('.\\Fecha.csv', ',')\n",
    "guardar_db(dest_db_connection_string, fecha,'Estudiante_19_202413.Fecha', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_producto_historia = '''(SELECT * FROM Estudiante_19_202413.ProductoHistoria) AS Tempo_producto_historia'''\n",
    "producto_historia = obtener_dataframe_de_bd(source_db_connection_string, sql_producto_historia, db_user, db_psswd)\n",
    "producto_historia.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto con toda la información transformada se va acrear la tabla de hechos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+---------------+---------------+------------------+--------+--------+-----------+---------+\n",
      "|ID_de_pedido_T|ID_Cliente_DWH|ID_Ciudad_DWH|ID_Empleado_DWH|ID_Producto_DWH|ID_TipoPaquete_DWH|ID_Fecha|Cantidad|Valor_total|Impuestos|\n",
      "+--------------+--------------+-------------+---------------+---------------+------------------+--------+--------+-----------+---------+\n",
      "|         72260|             0|            0|              1|             46|                 7|20160511|       8|        104|     1560|\n",
      "|         67020|            77|         3541|              4|  1580547965104|                 7|20160220|       8|       1840|    27600|\n",
      "|         66186|            22|        19238|              4|            221|                 7|20160204|      48|        720|    10800|\n",
      "|         72794|             0|            0|              6|            215|                 7|20160519|       9|      17091|   256365|\n",
      "|         68944|             0|            0|              9|              5|                 7|20160323|       2|         64|      960|\n",
      "+--------------+--------------+-------------+---------------+---------------+------------------+--------+--------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# El idPedido representa la dimensión degenerada Pedido\n",
    "ordenes = ordenes_tmp.alias('o').join(clientes.alias('cl'), ordenes_tmp.ID_de_cliente == clientes.ID_Cliente_T,'left_outer')\\\n",
    "                    .join(ciudades.alias('ciu'), clientes.ID_CiudadEntrega == ciudades.ID_ciudad_T,'left_outer') \\\n",
    "                    .join(empleados.alias('e'), ordenes_tmp.ID_de_vendedor == empleados.ID_Empleado_T,'left_outer') \\\n",
    "                    .join(paquetes.alias('p'), ordenes_tmp.ID_Tipo_Paquete == paquetes.ID_TipoPaquete_T,'left_outer') \\\n",
    "                    .join(producto_historia.alias('ph'), (ordenes_tmp.ID_Producto == producto_historia.ID_Producto_T) & (ordenes_tmp.Fecha_de_pedido.between(producto_historia.FechaInicial,producto_historia.FechaFinal)),'left_outer') \\\n",
    "                    .join(fecha.alias('f'), ordenes_tmp.Fecha_de_pedido == fecha.Fecha,'left_outer') \\\n",
    "                    .select([col('o.ID_de_pedido_T'),col('cl.ID_Cliente_DWH'),col('ciu.ID_Ciudad_DWH'),\n",
    "                             col('e.ID_Empleado_DWH'),col('ph.ID_Producto_DWH'),col('p.ID_TipoPaquete_DWH'),\n",
    "                             col('f.ID_Fecha'),col('o.Cantidad'),col('o.Valor_total'),col('o.Impuestos')]) \\\n",
    "                    .fillna({'ID_Cliente_DWH': 0, 'ID_Ciudad_DWH': 0, 'ID_Empleado_DWH': 0, 'ID_Producto_DWH': 0,\n",
    "                             'ID_TipoPaquete_DWH': 0})\n",
    "ordenes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23227"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordenes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta instrucción puede tomar un par de minutos.\n",
    "# Guarda la tabla de hechos\n",
    "guardar_db(dest_db_connection_string, ordenes,'Estudiante_19_202413.Hecho_Orden', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso de tener problema de memoria al momento de cargar una de las tablas, puede intentar un cargue por partes utilizando una idea como la que se presenta a continuación.\n",
    "# Revise con detalle el manejo de las variables por si requiere ajuste\n",
    "# inferior = 0\n",
    "# superior = 99999\n",
    "# j=0\n",
    "# total = ordenes.count()/100000\n",
    "# print(total)\n",
    "# collected = ordenes.collect()\n",
    "#while j<total:\n",
    "    #if j%50==0:\n",
    "    #    print(j)\n",
    "    #j += 1\n",
    "#    aux = spark.createDataFrame(collected[inferior:superior],ordenes.columns)\n",
    "#    guardar_db(dest_db_connection_string, aux,'Estudiante_i_2022XX.Hecho_Orden_tmp', db_user, db_psswd)\n",
    "#    inferior+=100000\n",
    "#    superior+=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultado de consultas\n",
    "Corresponde a las consultas realizadas sobre las tablas, para mostrar el estado final de las tablas pobladas como resultado del proceso de ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tarea ETL\n",
    "Espacio para desarrollar la tarea planteada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cierre\n",
    "Completado este tutorial, sabe cómo configurar y realizar ETLs con historia en PySpark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Información adicional\n",
    "\n",
    "Si quiere conocer más sobre PySpark la guía más detallada es la documentación oficial, la cual puede encontrar acá: https://spark.apache.org/docs/latest/api/python/index.html <br>\n",
    "Para ir directamente a la documentación de PySpark SQL, donde está la información sobre los DataFrames, haga clic en este enlace: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html <br>\n",
    "\n",
    "Para saber más sobre las técnicas de manejo de historia, consulte el libro <i>The  Data Warehouse Toolkit</i> de Ralph Kimball y Margy Ross,que podrá encontrar en la biblioteca de la universidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preguntas frecuentes\n",
    "\n",
    "- Si al intentar escribir un <i>dataframe</i> obtiene un error en el formato: \n",
    "    ```\n",
    "    path file:<PATH>/dw/<PATH> already exists.;\n",
    "    ```\n",
    "    Borre la carpeta indicada en el error y vuelva a intentar.\n",
    "\n",
    "- Si al ejecutar su código obtiene el error: \n",
    "    ```\n",
    "    ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=tutorial ETL PySpark, master=local) created by __init__ at <ipython-input-4-64455da959dd>:92 \n",
    "\n",
    "    ```\n",
    "    reinicie el kernel del notebook y vuelva a intentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
